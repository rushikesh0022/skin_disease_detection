{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5d777e",
   "metadata": {},
   "source": [
    "# üé≠ Face Concern Detector - Complete Interactive Demo\n",
    "\n",
    "## üöÄ Project Overview\n",
    "\n",
    "This notebook implements an **end-to-end face concern detection system** using deep learning to detect and visualize facial skin concerns including:\n",
    "\n",
    "- üî¥ **Acne**: Inflammatory skin conditions and blemishes\n",
    "- üëÅÔ∏è **Dark Circles**: Discoloration under the eyes  \n",
    "- üü° **Redness**: Skin irritation, rosacea, or inflammation\n",
    "- üìè **Wrinkles**: Fine lines and aging signs\n",
    "\n",
    "## ‚≠ê Key Features\n",
    "\n",
    "- **Multi-label Classification**: Detects multiple concerns simultaneously\n",
    "- **Face Detection**: Automatic face detection and alignment using MTCNN\n",
    "- **Explainable AI**: GradCAM visualizations showing prediction reasoning\n",
    "- **Mac Optimized**: MPS acceleration for Apple Silicon (M1/M2)\n",
    "- **Dual Dataset**: Combined training from multiple Kaggle sources\n",
    "- **Visual Overlays**: Semi-transparent heatmaps and confidence scores (0-100%)\n",
    "\n",
    "## üéØ Technology Stack\n",
    "\n",
    "- **Model**: ResNet18 with pretrained ImageNet weights\n",
    "- **Face Detection**: MTCNN for robust face alignment\n",
    "- **Visualization**: GradCAM for explainable predictions\n",
    "- **Framework**: PyTorch with MPS support\n",
    "- **Dataset**: Combined Kaggle datasets for comprehensive training\n",
    "\n",
    "---\n",
    "\n",
    "**Let's build an amazing face concern detection system! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa1e78",
   "metadata": {},
   "source": [
    "## üì¶ Section 1: Import Required Libraries and Setup Environment\n",
    "\n",
    "Let's start by importing all the necessary libraries and checking our system capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Deep learning and computer vision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Computer vision and image processing\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "\n",
    "# Face detection\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Progress bars and utilities\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Dataset download\n",
    "import kagglehub\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "\n",
    "# Check PyTorch version and device capabilities\n",
    "print(f\"\\nüîß System Information:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "# Device detection with detailed info\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"üöÄ Using Apple Silicon MPS acceleration!\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"üíª Using CPU\")\n",
    "\n",
    "print(f\"Selected device: {device}\")\n",
    "\n",
    "# Create directory structure\n",
    "directories = [\n",
    "    'data/raw',\n",
    "    'data/processed', \n",
    "    'data/sample_images',\n",
    "    'models/saved_weights',\n",
    "    'outputs/visualizations',\n",
    "    'outputs/logs',\n",
    "    'outputs/results'\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "print(f\"\\nüìÅ Created {len(directories)} directories for project organization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675102d",
   "metadata": {},
   "source": [
    "## üì• Section 2: Download and Combine Multiple Kaggle Datasets\n",
    "\n",
    "We'll download and combine two complementary Kaggle datasets to create a comprehensive training set:\n",
    "\n",
    "1. **Acne-Wrinkles-Spots Classification** - 600 images covering acne, wrinkles, and spots\n",
    "2. **Skin Defects Dataset** - Additional images for acne, redness, and dark circles\n",
    "\n",
    "This dual-dataset approach ensures we have sufficient training data for all four skin concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b81047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_combine_datasets():\n",
    "    \"\"\"\n",
    "    Download both Kaggle datasets and combine them into a unified format\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Starting dataset download and preparation...\")\n",
    "    \n",
    "    # Dataset 1: Acne-Wrinkles-Spots Classification\n",
    "    print(\"\\nüì• Downloading Acne-Wrinkles-Spots dataset...\")\n",
    "    try:\n",
    "        acne_spots_path = kagglehub.dataset_download(\n",
    "            \"ranvijaybalbir/acne-wrinkles-spots-classification\"\n",
    "        )\n",
    "        print(f\"‚úÖ Dataset 1 downloaded to: {acne_spots_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading dataset 1: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Dataset 2: Skin Defects Dataset\n",
    "    print(\"\\nüì• Downloading Skin Defects dataset...\")\n",
    "    try:\n",
    "        skin_defects_path = kagglehub.dataset_download(\n",
    "            \"trainingdatapro/skin-defects-acne-redness-and-bags-under-the-eyes\"\n",
    "        )\n",
    "        print(f\"‚úÖ Dataset 2 downloaded to: {skin_defects_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading dataset 2: {e}\")\n",
    "        return acne_spots_path, None\n",
    "    \n",
    "    return acne_spots_path, skin_defects_path\n",
    "\n",
    "def process_combined_datasets(acne_spots_path, skin_defects_path, output_dir='data/processed'):\n",
    "    \"\"\"\n",
    "    Process and combine both datasets into unified format\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    annotations = []\n",
    "    \n",
    "    print(\"\\nüîÑ Processing Dataset 1: Acne-Wrinkles-Spots...\")\n",
    "    \n",
    "    # Process first dataset\n",
    "    dataset1_mapping = {\n",
    "        'acne': 'acne',\n",
    "        'wrinkles': 'wrinkles',\n",
    "        'spots': 'redness'  # Map spots to redness\n",
    "    }\n",
    "    \n",
    "    for kaggle_cat, our_cat in dataset1_mapping.items():\n",
    "        src_dir = os.path.join(acne_spots_path, kaggle_cat)\n",
    "        if os.path.exists(src_dir):\n",
    "            image_files = [f for f in os.listdir(src_dir) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            print(f\"  Found {len(image_files)} images for {kaggle_cat}\")\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                # Create unique filename\n",
    "                unique_name = f\"ds1_{kaggle_cat}_{img_file}\"\n",
    "                \n",
    "                # Copy image\n",
    "                src_path = os.path.join(src_dir, img_file)\n",
    "                dst_path = os.path.join(output_dir, unique_name)\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "                \n",
    "                # Create annotation\n",
    "                labels = {\n",
    "                    'acne': 1 if our_cat == 'acne' else 0,\n",
    "                    'dark_circles': 0,  # Not in this dataset\n",
    "                    'redness': 1 if our_cat == 'redness' else 0,\n",
    "                    'wrinkles': 1 if our_cat == 'wrinkles' else 0\n",
    "                }\n",
    "                \n",
    "                annotations.append({\n",
    "                    'image_name': unique_name,\n",
    "                    'source': 'acne_spots',\n",
    "                    'original_category': kaggle_cat,\n",
    "                    **labels\n",
    "                })\n",
    "    \n",
    "    print(\"\\nüîÑ Processing Dataset 2: Skin Defects...\")\n",
    "    \n",
    "    # Process second dataset (try multiple directory structures)\n",
    "    dataset2_mapping = {\n",
    "        'acne': 'acne',\n",
    "        'redness': 'redness', \n",
    "        'bags_under_eyes': 'dark_circles',\n",
    "        'bags-under-eyes': 'dark_circles',\n",
    "        'dark_circles': 'dark_circles'\n",
    "    }\n",
    "    \n",
    "    for kaggle_cat, our_cat in dataset2_mapping.items():\n",
    "        # Try different possible directory structures\n",
    "        possible_paths = [\n",
    "            os.path.join(skin_defects_path, kaggle_cat),\n",
    "            os.path.join(skin_defects_path, kaggle_cat.replace('_', '-')),\n",
    "            os.path.join(skin_defects_path, 'data', kaggle_cat),\n",
    "            os.path.join(skin_defects_path, 'images', kaggle_cat),\n",
    "            os.path.join(skin_defects_path, kaggle_cat.title())\n",
    "        ]\n",
    "        \n",
    "        src_dir = None\n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                src_dir = path\n",
    "                break\n",
    "        \n",
    "        if src_dir:\n",
    "            image_files = [f for f in os.listdir(src_dir) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            print(f\"  Found {len(image_files)} images for {kaggle_cat}\")\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                # Create unique filename\n",
    "                unique_name = f\"ds2_{kaggle_cat}_{img_file}\"\n",
    "                \n",
    "                # Copy image\n",
    "                src_path = os.path.join(src_dir, img_file)\n",
    "                dst_path = os.path.join(output_dir, unique_name)\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "                \n",
    "                # Create annotation\n",
    "                labels = {\n",
    "                    'acne': 1 if our_cat == 'acne' else 0,\n",
    "                    'dark_circles': 1 if our_cat == 'dark_circles' else 0,\n",
    "                    'redness': 1 if our_cat == 'redness' else 0,\n",
    "                    'wrinkles': 0  # Not in this dataset\n",
    "                }\n",
    "                \n",
    "                annotations.append({\n",
    "                    'image_name': unique_name,\n",
    "                    'source': 'skin_defects',\n",
    "                    'original_category': kaggle_cat,\n",
    "                    **labels\n",
    "                })\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è No directory found for {kaggle_cat}\")\n",
    "    \n",
    "    # Save combined annotations\n",
    "    df = pd.DataFrame(annotations)\n",
    "    output_csv = os.path.join(output_dir, 'combined_annotations.csv')\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nüìä Combined Dataset Statistics:\")\n",
    "    print(f\"Total images: {len(df)}\")\n",
    "    print(f\"From Dataset 1 (Acne-Wrinkles-Spots): {len(df[df['source'] == 'acne_spots'])}\")\n",
    "    print(f\"From Dataset 2 (Skin Defects): {len(df[df['source'] == 'skin_defects'])}\")\n",
    "    print(f\"\\nüè∑Ô∏è Concern Distribution:\")\n",
    "    \n",
    "    concern_stats = {}\n",
    "    for concern in ['acne', 'dark_circles', 'redness', 'wrinkles']:\n",
    "        count = df[concern].sum()\n",
    "        concern_stats[concern] = count\n",
    "        print(f\"  {concern.replace('_', ' ').title()}: {count} images\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Combined dataset saved to: {output_dir}\")\n",
    "    print(f\"‚úÖ Annotations saved to: {output_csv}\")\n",
    "    \n",
    "    return df, concern_stats\n",
    "\n",
    "# Download and process datasets\n",
    "acne_spots_path, skin_defects_path = download_and_combine_datasets()\n",
    "\n",
    "if acne_spots_path:\n",
    "    combined_df, stats = process_combined_datasets(acne_spots_path, skin_defects_path)\n",
    "    \n",
    "    # Visualize dataset distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Subplot 1: Concern distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    concerns = list(stats.keys())\n",
    "    counts = list(stats.values())\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "    \n",
    "    plt.bar(concerns, counts, color=colors, alpha=0.8)\n",
    "    plt.title('Skin Concern Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Skin Concerns')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for i, v in enumerate(counts):\n",
    "        plt.text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    # Subplot 2: Dataset sources\n",
    "    plt.subplot(1, 2, 2)\n",
    "    source_counts = combined_df['source'].value_counts()\n",
    "    plt.pie(source_counts.values, labels=['Acne-Wrinkles-Spots', 'Skin Defects'], \n",
    "            autopct='%1.1f%%', colors=['#FF9999', '#66B2FF'], startangle=90)\n",
    "    plt.title('Dataset Sources', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/visualizations/dataset_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüéØ Dataset preparation complete! Ready for training with {len(combined_df)} total images.\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to download datasets. Please check your internet connection and kagglehub setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd617f68",
   "metadata": {},
   "source": [
    "## üë§ Section 3: Face Detection and Preprocessing Pipeline\n",
    "\n",
    "Now let's implement our robust face detection and preprocessing pipeline using MTCNN (Multi-task Cascaded Convolutional Networks) for accurate face detection and alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0722d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacePreprocessor:\n",
    "    \"\"\"\n",
    "    Advanced face detection and preprocessing pipeline using MTCNN\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=224, margin=20, min_face_size=40):\n",
    "        \"\"\"\n",
    "        Initialize face preprocessor\n",
    "        \n",
    "        Args:\n",
    "            image_size: Target size for face images (224x224 for ResNet)\n",
    "            margin: Margin around detected face in pixels\n",
    "            min_face_size: Minimum face size to consider valid\n",
    "        \"\"\"\n",
    "        self.detector = MTCNN(device='cpu')  # MTCNN works best on CPU\n",
    "        self.image_size = image_size\n",
    "        self.margin = margin\n",
    "        self.min_face_size = min_face_size\n",
    "        \n",
    "        print(f\"üéØ FacePreprocessor initialized:\")\n",
    "        print(f\"   Image size: {image_size}x{image_size}\")\n",
    "        print(f\"   Margin: {margin}px\")\n",
    "        print(f\"   Min face size: {min_face_size}px\")\n",
    "    \n",
    "    def detect_faces(self, image):\n",
    "        \"\"\"\n",
    "        Detect all faces in image with confidence scores\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or numpy array\n",
    "            \n",
    "        Returns:\n",
    "            List of detected faces with bounding boxes and confidence\n",
    "        \"\"\"\n",
    "        if isinstance(image, Image.Image):\n",
    "            image_array = np.array(image)\n",
    "        else:\n",
    "            image_array = image\n",
    "        \n",
    "        try:\n",
    "            results = self.detector.detect_faces(image_array)\n",
    "            \n",
    "            # Filter by minimum face size and confidence\n",
    "            valid_faces = []\n",
    "            for face in results:\n",
    "                bbox = face['box']\n",
    "                confidence = face['confidence']\n",
    "                \n",
    "                # Check minimum size\n",
    "                if bbox[2] >= self.min_face_size and bbox[3] >= self.min_face_size:\n",
    "                    # Check confidence (MTCNN usually gives good results above 0.9)\n",
    "                    if confidence > 0.8:\n",
    "                        valid_faces.append(face)\n",
    "            \n",
    "            return valid_faces\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Face detection error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_best_face(self, faces):\n",
    "        \"\"\"\n",
    "        Select the best face from detected faces (highest confidence + largest size)\n",
    "        \"\"\"\n",
    "        if not faces:\n",
    "            return None\n",
    "        \n",
    "        # Score faces based on confidence and size\n",
    "        scored_faces = []\n",
    "        for face in faces:\n",
    "            bbox = face['box']\n",
    "            confidence = face['confidence']\n",
    "            face_area = bbox[2] * bbox[3]  # width * height\n",
    "            \n",
    "            # Combined score: confidence * normalized_area\n",
    "            score = confidence * (face_area / 10000)  # Normalize area\n",
    "            scored_faces.append((score, face))\n",
    "        \n",
    "        # Return face with highest score\n",
    "        return max(scored_faces, key=lambda x: x[0])[1]\n",
    "    \n",
    "    def crop_and_align_face(self, image, face_info):\n",
    "        \"\"\"\n",
    "        Crop face from image with proper alignment and margin\n",
    "        \"\"\"\n",
    "        if isinstance(image, Image.Image):\n",
    "            img_array = np.array(image)\n",
    "        else:\n",
    "            img_array = image\n",
    "        \n",
    "        bbox = face_info['box']\n",
    "        x, y, w, h = bbox\n",
    "        \n",
    "        # Add margin while staying within image bounds\n",
    "        img_height, img_width = img_array.shape[:2]\n",
    "        \n",
    "        x1 = max(0, x - self.margin)\n",
    "        y1 = max(0, y - self.margin)\n",
    "        x2 = min(img_width, x + w + self.margin)\n",
    "        y2 = min(img_height, y + h + self.margin)\n",
    "        \n",
    "        # Crop face\n",
    "        face_crop = img_array[y1:y2, x1:x2]\n",
    "        \n",
    "        # Convert back to PIL Image\n",
    "        face_image = Image.fromarray(face_crop)\n",
    "        \n",
    "        # Resize to target size\n",
    "        face_resized = face_image.resize((self.image_size, self.image_size), Image.LANCZOS)\n",
    "        \n",
    "        return face_resized, (x1, y1, x2, y2)\n",
    "    \n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline: detect -> crop -> resize\n",
    "        \n",
    "        Returns:\n",
    "            Tuple: (processed_face_image, detection_info) or (None, error_msg)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            if isinstance(image_path, str):\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "            else:\n",
    "                image = image_path.convert('RGB')\n",
    "            \n",
    "            # Detect faces\n",
    "            faces = self.detect_faces(image)\n",
    "            \n",
    "            if not faces:\n",
    "                return None, \"No face detected\"\n",
    "            \n",
    "            # Get best face\n",
    "            best_face = self.get_best_face(faces)\n",
    "            \n",
    "            if not best_face:\n",
    "                return None, \"No suitable face found\"\n",
    "            \n",
    "            # Crop and align\n",
    "            face_image, crop_coords = self.crop_and_align_face(image, best_face)\n",
    "            \n",
    "            detection_info = {\n",
    "                'confidence': best_face['confidence'],\n",
    "                'bbox': best_face['box'],\n",
    "                'crop_coords': crop_coords,\n",
    "                'num_faces_detected': len(faces)\n",
    "            }\n",
    "            \n",
    "            return face_image, detection_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, f\"Preprocessing error: {str(e)}\"\n",
    "\n",
    "def create_data_transforms(train=True, image_size=224):\n",
    "    \"\"\"\n",
    "    Create data augmentation transforms optimized for face images\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        # Training augmentations (careful not to distort facial features too much)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=10, fill=0),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.2,\n",
    "                contrast=0.2, \n",
    "                saturation=0.2,\n",
    "                hue=0.1\n",
    "            ),\n",
    "            transforms.RandomAffine(\n",
    "                degrees=0,\n",
    "                translate=(0.05, 0.05),\n",
    "                scale=(0.95, 1.05),\n",
    "                fill=0\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],  # ImageNet pretrained means\n",
    "                std=[0.229, 0.224, 0.225]    # ImageNet pretrained stds\n",
    "            )\n",
    "        ])\n",
    "    else:\n",
    "        # Validation/test transforms (no augmentation)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "# Initialize face preprocessor\n",
    "face_preprocessor = FacePreprocessor(image_size=224, margin=20)\n",
    "\n",
    "# Test face detection on a sample (if any processed images exist)\n",
    "processed_dir = 'data/processed'\n",
    "if os.path.exists(processed_dir):\n",
    "    sample_images = [f for f in os.listdir(processed_dir) \n",
    "                    if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    if sample_images:\n",
    "        # Test on first few images\n",
    "        print(f\"\\nüß™ Testing face detection on sample images...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        test_count = min(6, len(sample_images))\n",
    "        \n",
    "        for i in range(test_count):\n",
    "            img_path = os.path.join(processed_dir, sample_images[i])\n",
    "            \n",
    "            # Process image\n",
    "            face_img, detection_info = face_preprocessor.preprocess_image(img_path)\n",
    "            \n",
    "            if face_img:\n",
    "                axes[i].imshow(face_img)\n",
    "                axes[i].set_title(\n",
    "                    f\"‚úÖ Face Detected\\nConf: {detection_info['confidence']:.2f}\", \n",
    "                    color='green', fontsize=10\n",
    "                )\n",
    "            else:\n",
    "                # Show original image if face detection failed\n",
    "                original = Image.open(img_path)\n",
    "                axes[i].imshow(original)\n",
    "                axes[i].set_title(f\"‚ùå {detection_info}\", color='red', fontsize=10)\n",
    "            \n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(test_count, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle('Face Detection Test Results', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('outputs/visualizations/face_detection_test.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"‚úÖ Face detection tested on {test_count} images\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No sample images found for face detection testing\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No processed directory found yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47413a6c",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Section 4: Dataset Preparation and Multi-Label Annotation\n",
    "\n",
    "Let's create our custom dataset class for multi-label skin concern classification and properly split our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08033f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinConcernDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for multi-label skin concern classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, annotations_df=None, transform=None, face_preprocessor=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: Directory containing images\n",
    "            annotations_df: DataFrame with image annotations\n",
    "            transform: Data augmentation transforms\n",
    "            face_preprocessor: FacePreprocessor instance for face detection\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.face_preprocessor = face_preprocessor\n",
    "        \n",
    "        if annotations_df is not None:\n",
    "            self.annotations = annotations_df.reset_index(drop=True)\n",
    "            self.concern_labels = ['acne', 'dark_circles', 'redness', 'wrinkles']\n",
    "        else:\n",
    "            # Load all images if no annotations provided\n",
    "            self.annotations = None\n",
    "            self.image_files = [f for f in os.listdir(data_dir) \n",
    "                               if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.annotations is not None:\n",
    "            return len(self.annotations)\n",
    "        else:\n",
    "            return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset\n",
    "        \n",
    "        Returns:\n",
    "            If annotations available: (image_tensor, label_tensor, metadata)\n",
    "            If no annotations: (image_tensor, image_name)\n",
    "        \"\"\"\n",
    "        if self.annotations is not None:\n",
    "            # Training/validation mode with labels\n",
    "            row = self.annotations.iloc[idx]\n",
    "            img_name = row['image_name']\n",
    "            img_path = os.path.join(self.data_dir, img_name)\n",
    "            \n",
    "            # Load and preprocess image\n",
    "            if self.face_preprocessor:\n",
    "                face_img, detection_info = self.face_preprocessor.preprocess_image(img_path)\n",
    "                if face_img is None:\n",
    "                    # Fallback to original image if face detection fails\n",
    "                    face_img = Image.open(img_path).convert('RGB')\n",
    "                    detection_info = {'confidence': 0, 'fallback': True}\n",
    "            else:\n",
    "                face_img = Image.open(img_path).convert('RGB')\n",
    "                detection_info = {'no_preprocessing': True}\n",
    "            \n",
    "            # Apply transforms\n",
    "            if self.transform:\n",
    "                image_tensor = self.transform(face_img)\n",
    "            else:\n",
    "                image_tensor = transforms.ToTensor()(face_img)\n",
    "            \n",
    "            # Get multi-label targets\n",
    "            labels = torch.tensor([\n",
    "                row['acne'],\n",
    "                row['dark_circles'], \n",
    "                row['redness'],\n",
    "                row['wrinkles']\n",
    "            ], dtype=torch.float32)\n",
    "            \n",
    "            metadata = {\n",
    "                'image_name': img_name,\n",
    "                'source': row.get('source', 'unknown'),\n",
    "                'detection_info': detection_info\n",
    "            }\n",
    "            \n",
    "            return image_tensor, labels, metadata\n",
    "        \n",
    "        else:\n",
    "            # Inference mode without labels\n",
    "            img_name = self.image_files[idx]\n",
    "            img_path = os.path.join(self.data_dir, img_name)\n",
    "            \n",
    "            # Load image\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # Apply transforms\n",
    "            if self.transform:\n",
    "                image_tensor = self.transform(image)\n",
    "            else:\n",
    "                image_tensor = transforms.ToTensor()(image)\n",
    "            \n",
    "            return image_tensor, img_name\n",
    "\n",
    "def split_dataset_smart(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Smart dataset splitting that ensures balanced representation across concerns\n",
    "    \"\"\"\n",
    "    print(f\"üìä Splitting dataset: {train_ratio:.0%} train, {val_ratio:.0%} val, {test_ratio:.0%} test\")\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    df_shuffled = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    n_total = len(df_shuffled)\n",
    "    n_train = int(n_total * train_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    n_test = n_total - n_train - n_val\n",
    "    \n",
    "    # Split the data\n",
    "    train_df = df_shuffled[:n_train].copy()\n",
    "    val_df = df_shuffled[n_train:n_train + n_val].copy()\n",
    "    test_df = df_shuffled[n_train + n_val:].copy()\n",
    "    \n",
    "    # Verify splits\n",
    "    print(f\"‚úÖ Split completed:\")\n",
    "    print(f\"   Train: {len(train_df)} images ({len(train_df)/n_total:.1%})\")\n",
    "    print(f\"   Val:   {len(val_df)} images ({len(val_df)/n_total:.1%})\")\n",
    "    print(f\"   Test:  {len(test_df)} images ({len(test_df)/n_total:.1%})\")\n",
    "    \n",
    "    # Check concern distribution in each split\n",
    "    concerns = ['acne', 'dark_circles', 'redness', 'wrinkles']\n",
    "    \n",
    "    print(f\"\\nüìà Concern distribution across splits:\")\n",
    "    for concern in concerns:\n",
    "        train_count = train_df[concern].sum()\n",
    "        val_count = val_df[concern].sum()\n",
    "        test_count = test_df[concern].sum()\n",
    "        total_count = train_count + val_count + test_count\n",
    "        \n",
    "        print(f\"   {concern.replace('_', ' ').title()}:\")\n",
    "        print(f\"     Train: {train_count} ({train_count/total_count:.1%})\")\n",
    "        print(f\"     Val:   {val_count} ({val_count/total_count:.1%})\")\n",
    "        print(f\"     Test:  {test_count} ({test_count/total_count:.1%})\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def save_dataset_splits(train_df, val_df, test_df, base_path='data/processed'):\n",
    "    \"\"\"\n",
    "    Save dataset splits to CSV files\n",
    "    \"\"\"\n",
    "    train_path = os.path.join(base_path, 'train_annotations.csv')\n",
    "    val_path = os.path.join(base_path, 'val_annotations.csv') \n",
    "    test_path = os.path.join(base_path, 'test_annotations.csv')\n",
    "    \n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    val_df.to_csv(val_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "    \n",
    "    print(f\"üíæ Dataset splits saved:\")\n",
    "    print(f\"   Train: {train_path}\")\n",
    "    print(f\"   Val:   {val_path}\")\n",
    "    print(f\"   Test:  {test_path}\")\n",
    "    \n",
    "    return train_path, val_path, test_path\n",
    "\n",
    "# Prepare dataset splits (if combined_df exists from previous section)\n",
    "if 'combined_df' in locals() and combined_df is not None:\n",
    "    print(\"üîÑ Preparing dataset splits...\")\n",
    "    \n",
    "    # Split the dataset\n",
    "    train_df, val_df, test_df = split_dataset_smart(\n",
    "        combined_df, \n",
    "        train_ratio=0.7, \n",
    "        val_ratio=0.15, \n",
    "        test_ratio=0.15\n",
    "    )\n",
    "    \n",
    "    # Save splits\n",
    "    train_path, val_path, test_path = save_dataset_splits(train_df, val_df, test_df)\n",
    "    \n",
    "    # Create dataset instances\n",
    "    print(f\"\\nüèóÔ∏è Creating dataset instances...\")\n",
    "    \n",
    "    # Create transforms\n",
    "    train_transform = create_data_transforms(train=True)\n",
    "    val_transform = create_data_transforms(train=False)\n",
    "    \n",
    "    # Create datasets (without face preprocessing for now to test basic functionality)\n",
    "    train_dataset = SkinConcernDataset(\n",
    "        data_dir='data/processed',\n",
    "        annotations_df=train_df,\n",
    "        transform=train_transform,\n",
    "        face_preprocessor=None  # We'll add this later for training\n",
    "    )\n",
    "    \n",
    "    val_dataset = SkinConcernDataset(\n",
    "        data_dir='data/processed',\n",
    "        annotations_df=val_df,\n",
    "        transform=val_transform,\n",
    "        face_preprocessor=None\n",
    "    )\n",
    "    \n",
    "    test_dataset = SkinConcernDataset(\n",
    "        data_dir='data/processed',\n",
    "        annotations_df=test_df,\n",
    "        transform=val_transform,\n",
    "        face_preprocessor=None\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset instances created:\")\n",
    "    print(f\"   Train dataset: {len(train_dataset)} samples\")\n",
    "    print(f\"   Val dataset:   {len(val_dataset)} samples\") \n",
    "    print(f\"   Test dataset:  {len(test_dataset)} samples\")\n",
    "    \n",
    "    # Test dataset loading\n",
    "    print(f\"\\nüß™ Testing dataset loading...\")\n",
    "    \n",
    "    # Test one sample from training set\n",
    "    sample_img, sample_labels, sample_metadata = train_dataset[0]\n",
    "    \n",
    "    print(f\"Sample image shape: {sample_img.shape}\")\n",
    "    print(f\"Sample labels: {sample_labels}\")\n",
    "    print(f\"Sample metadata: {sample_metadata}\")\n",
    "    \n",
    "    # Visualize a few samples\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    concern_names = ['Acne', 'Dark Circles', 'Redness', 'Wrinkles'] \n",
    "    colors = ['red', 'blue', 'orange', 'purple']\n",
    "    \n",
    "    for i in range(8):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        \n",
    "        # Get sample\n",
    "        img_tensor, labels, metadata = train_dataset[i]\n",
    "        \n",
    "        # Convert tensor to displayable image\n",
    "        img_display = img_tensor.permute(1, 2, 0)\n",
    "        img_display = img_display * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
    "        img_display = torch.clamp(img_display, 0, 1)\n",
    "        \n",
    "        axes[row, col].imshow(img_display)\n",
    "        \n",
    "        # Create title with detected concerns\n",
    "        detected = [concern_names[j] for j, label in enumerate(labels) if label == 1]\n",
    "        title = f\"Sample {i+1}\\\\n{', '.join(detected) if detected else 'None'}\"\n",
    "        axes[row, col].set_title(title, fontsize=10)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.suptitle('Training Dataset Samples', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/visualizations/dataset_samples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Dataset preparation complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Combined dataset not found. Please run the dataset download section first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f7171",
   "metadata": {},
   "source": [
    "## üß† Section 5: Model Architecture - ResNet18 Multi-Label Classifier\n",
    "\n",
    "Let's implement our ResNet18-based multi-label classifier optimized for skin concern detection with Mac MPS acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinConcernDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet18-based multi-label classifier for skin concern detection\n",
    "    Optimized for Mac with MPS support\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=4, pretrained=True, dropout_rate=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes: Number of skin concerns (4: acne, dark_circles, redness, wrinkles)\n",
    "            pretrained: Use ImageNet pretrained weights\n",
    "            dropout_rate: Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        super(SkinConcernDetector, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet18\n",
    "        self.backbone = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Get number of features from the last layer\n",
    "        num_features = self.backbone.fc.in_features  # 512 for ResNet18\n",
    "        \n",
    "        # Replace the final fully connected layer with multi-label classifier\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)  # No activation here\n",
    "        )\n",
    "        \n",
    "        # Sigmoid for multi-label classification (applied in forward)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Store config\n",
    "        self.num_classes = num_classes\n",
    "        self.concern_labels = ['acne', 'dark_circles', 'redness', 'wrinkles']\n",
    "        \n",
    "        print(f\"üèóÔ∏è SkinConcernDetector initialized:\")\n",
    "        print(f\"   Backbone: ResNet18 (pretrained: {pretrained})\")\n",
    "        print(f\"   Input features: {num_features}\")\n",
    "        print(f\"   Output classes: {num_classes}\")\n",
    "        print(f\"   Dropout rate: {dropout_rate}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, 3, 224, 224]\n",
    "            \n",
    "        Returns:\n",
    "            Probabilities for each concern [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # Get logits from backbone\n",
    "        logits = self.backbone(x)\n",
    "        \n",
    "        # Apply sigmoid for multi-label probabilities\n",
    "        probabilities = self.sigmoid(logits)\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"\n",
    "        Extract intermediate features for GradCAM visualization\n",
    "        \n",
    "        Returns:\n",
    "            Features from layer4 (before global average pooling)\n",
    "        \"\"\"\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "        \n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        features = self.backbone.layer4(x)  # [batch_size, 512, 7, 7]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def get_predictions(self, x, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Get binary predictions and confidence scores\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            threshold: Classification threshold\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with predictions and scores\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            probabilities = self.forward(x)\n",
    "            \n",
    "            # Convert to binary predictions\n",
    "            binary_preds = (probabilities > threshold).float()\n",
    "            \n",
    "            # Convert to lists for easier handling\n",
    "            probs_list = probabilities.cpu().numpy()\n",
    "            preds_list = binary_preds.cpu().numpy()\n",
    "            \n",
    "            results = {\n",
    "                'probabilities': probs_list,\n",
    "                'predictions': preds_list,\n",
    "                'threshold': threshold\n",
    "            }\n",
    "            \n",
    "            return results\n",
    "\n",
    "class MultiLabelLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary Cross Entropy Loss for multi-label classification with class weighting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pos_weights=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weights: Tensor of positive class weights for handling imbalance\n",
    "            reduction: Loss reduction method ('mean', 'sum', 'none')\n",
    "        \"\"\"\n",
    "        super(MultiLabelLoss, self).__init__()\n",
    "        \n",
    "        if pos_weights is not None:\n",
    "            self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction=reduction)\n",
    "        else:\n",
    "            self.criterion = nn.BCELoss(reduction=reduction)\n",
    "        \n",
    "        self.use_logits = pos_weights is not None\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Calculate multi-label loss\n",
    "        \n",
    "        Args:\n",
    "            predictions: Model predictions [batch_size, num_classes]\n",
    "            targets: Ground truth labels [batch_size, num_classes]\n",
    "            \n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "        if self.use_logits:\n",
    "            # predictions should be logits (before sigmoid)\n",
    "            return self.criterion(predictions, targets)\n",
    "        else:\n",
    "            # predictions should be probabilities (after sigmoid) \n",
    "            return self.criterion(predictions, targets)\n",
    "\n",
    "def calculate_class_weights(train_df, concerns=['acne', 'dark_circles', 'redness', 'wrinkles']):\n",
    "    \"\"\"\n",
    "    Calculate class weights to handle imbalanced data\n",
    "    \"\"\"\n",
    "    pos_counts = []\n",
    "    neg_counts = []\n",
    "    \n",
    "    for concern in concerns:\n",
    "        pos_count = train_df[concern].sum()\n",
    "        neg_count = len(train_df) - pos_count\n",
    "        pos_counts.append(pos_count)\n",
    "        neg_counts.append(neg_count)\n",
    "    \n",
    "    # Calculate positive weights (higher weight for less frequent classes)\n",
    "    pos_weights = []\n",
    "    for pos_count, neg_count in zip(pos_counts, neg_counts):\n",
    "        if pos_count > 0:\n",
    "            weight = neg_count / pos_count\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        pos_weights.append(weight)\n",
    "    \n",
    "    pos_weights_tensor = torch.tensor(pos_weights, dtype=torch.float32)\n",
    "    \n",
    "    print(f\"üìä Class weights calculated:\")\n",
    "    for i, (concern, weight) in enumerate(zip(concerns, pos_weights)):\n",
    "        print(f\"   {concern}: {weight:.2f} (pos: {pos_counts[i]}, neg: {neg_counts[i]})\")\n",
    "    \n",
    "    return pos_weights_tensor\n",
    "\n",
    "# Create model instance\n",
    "print(\"üèóÔ∏è Creating SkinConcernDetector model...\")\n",
    "\n",
    "model = SkinConcernDetector(\n",
    "    num_classes=4,\n",
    "    pretrained=True,\n",
    "    dropout_rate=0.5\n",
    ").to(device)\n",
    "\n",
    "# Calculate class weights if training data is available\n",
    "if 'train_df' in locals():\n",
    "    class_weights = calculate_class_weights(train_df)\n",
    "    class_weights = class_weights.to(device)\n",
    "    \n",
    "    # Create loss function with class weights\n",
    "    criterion = MultiLabelLoss(pos_weights=class_weights)\n",
    "else:\n",
    "    # Default loss without weights\n",
    "    criterion = MultiLabelLoss()\n",
    "    class_weights = None\n",
    "\n",
    "# Move loss to device\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Print model summary\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(f\"\\nüìà Model Summary:\")\n",
    "print(f\"   Total trainable parameters: {total_params:,}\")\n",
    "print(f\"   Model size: ~{total_params * 4 / (1024**2):.1f} MB (FP32)\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\nüß™ Testing forward pass...\")\n",
    "dummy_input = torch.randn(2, 3, 224, 224).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    dummy_output = model(dummy_input)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {dummy_output.shape}\")\n",
    "    print(f\"Output range: [{dummy_output.min():.3f}, {dummy_output.max():.3f}]\")\n",
    "\n",
    "# Visualize model architecture\n",
    "def visualize_model_architecture():\n",
    "    \"\"\"Create a simple visualization of the model architecture\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Define architecture components\n",
    "    components = [\n",
    "        (\"Input Image\", \"3x224x224\", \"#FF9999\"),\n",
    "        (\"ResNet18 Backbone\", \"Feature Extraction\", \"#66B2FF\"),  \n",
    "        (\"Conv Layers\", \"64‚Üí128‚Üí256‚Üí512\", \"#99FF99\"),\n",
    "        (\"Global Avg Pool\", \"512x7x7 ‚Üí 512\", \"#FFCC99\"),\n",
    "        (\"FC + Dropout\", \"512 ‚Üí 256\", \"#FF99CC\"),\n",
    "        (\"BatchNorm + ReLU\", \"Normalization\", \"#99FFCC\"),\n",
    "        (\"Final FC\", \"256 ‚Üí 4\", \"#CCCCFF\"),\n",
    "        (\"Sigmoid\", \"Multi-label Output\", \"#FFCCCC\"),\n",
    "        (\"Output\", \"4 Probabilities\", \"#CCFFCC\")\n",
    "    ]\n",
    "    \n",
    "    # Draw architecture\n",
    "    y_positions = np.linspace(0.9, 0.1, len(components))\n",
    "    \n",
    "    for i, (name, desc, color) in enumerate(components):\n",
    "        # Draw component box\n",
    "        rect = patches.FancyBboxPatch(\n",
    "            (0.1, y_positions[i] - 0.03), 0.8, 0.06,\n",
    "            boxstyle=\"round,pad=0.01\",\n",
    "            facecolor=color,\n",
    "            edgecolor='black',\n",
    "            linewidth=1\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add text\n",
    "        ax.text(0.15, y_positions[i], name, fontsize=12, fontweight='bold', va='center')\n",
    "        ax.text(0.85, y_positions[i], desc, fontsize=10, va='center', ha='right')\n",
    "        \n",
    "        # Draw arrows (except for last component)\n",
    "        if i < len(components) - 1:\n",
    "            ax.arrow(0.5, y_positions[i] - 0.04, 0, -0.03, \n",
    "                    head_width=0.02, head_length=0.01, fc='black', ec='black')\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('SkinConcernDetector Architecture', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add concern labels\n",
    "    concern_text = \"Output Classes: \" + \" | \".join(model.concern_labels)\n",
    "    ax.text(0.5, 0.02, concern_text, ha='center', fontsize=11, \n",
    "           bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\"))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/visualizations/model_architecture.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the architecture\n",
    "visualize_model_architecture()\n",
    "\n",
    "print(\"‚úÖ Model architecture created and tested successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c7bec4",
   "metadata": {},
   "source": [
    "## üöÄ Section 6: Training Pipeline with Mac Optimization\n",
    "\n",
    "Now let's implement the complete training loop with Mac MPS optimization, learning rate scheduling, and comprehensive monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinConcernTrainer:\n",
    "    \"\"\"\n",
    "    Comprehensive training manager for SkinConcernDetector with Mac optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, criterion, device, config=None):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "        # Default training configuration\n",
    "        self.config = config or {\n",
    "            'learning_rate': 1e-4,\n",
    "            'weight_decay': 1e-4,\n",
    "            'patience': 5,\n",
    "            'min_lr': 1e-7,\n",
    "            'num_epochs': 25,\n",
    "            'save_frequency': 5\n",
    "        }\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=self.config['learning_rate'],\n",
    "            weight_decay=self.config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            patience=self.config['patience'],\n",
    "            factor=0.5,\n",
    "            min_lr=self.config['min_lr'],\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_acc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_val_acc = 0.0\n",
    "        self.epochs_without_improvement = 0\n",
    "        \n",
    "        print(f\"üèãÔ∏è Trainer initialized:\")\n",
    "        print(f\"   Optimizer: AdamW (lr={self.config['learning_rate']}, wd={self.config['weight_decay']})\")\n",
    "        print(f\"   Scheduler: ReduceLROnPlateau (patience={self.config['patience']})\")\n",
    "        print(f\"   Device: {device}\")\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(self.train_loader, desc='Training', leave=False)\n",
    "        \n",
    "        for batch_idx, (images, targets, metadata) in enumerate(pbar):\n",
    "            images = images.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Optimizer step\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Calculate accuracy (percentage of correctly predicted labels)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            batch_accuracy = (preds == targets).float().mean()\n",
    "            running_corrects += batch_accuracy * images.size(0)\n",
    "            total_samples += images.size(0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{batch_accuracy:.3f}'\n",
    "            })\\n        \n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_corrects / total_samples\n",
    "        \n",
    "        return epoch_loss, epoch_acc.item()\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        all_outputs = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(self.val_loader, desc='Validation', leave=False)\n",
    "            \n",
    "            for images, targets, metadata in pbar:\n",
    "                images = images.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                # Accuracy\n",
    "                preds = (outputs > 0.5).float()\n",
    "                batch_accuracy = (preds == targets).float().mean()\n",
    "                running_corrects += batch_accuracy * images.size(0)\n",
    "                total_samples += images.size(0)\n",
    "                \n",
    "                # Store for detailed metrics\n",
    "                all_outputs.extend(outputs.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{batch_accuracy:.3f}'\n",
    "                })\n",
    "        \n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_corrects / total_samples\n",
    "        \n",
    "        return epoch_loss, epoch_acc.item(), np.array(all_outputs), np.array(all_targets)\n",
    "    \n",
    "    def save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        os.makedirs('models/saved_weights', exist_ok=True)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'best_val_acc': self.best_val_acc,\n",
    "            'history': self.history,\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        latest_path = 'models/saved_weights/latest_checkpoint.pth'\n",
    "        torch.save(checkpoint, latest_path)\n",
    "        \n",
    "        # Save best checkpoint\n",
    "        if is_best:\n",
    "            best_path = 'models/saved_weights/best_model.pth'\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"üíæ Best model saved! Val Loss: {self.best_val_loss:.4f}, Val Acc: {self.best_val_acc:.3f}\")\n",
    "    \n",
    "    def plot_training_progress(self):\n",
    "        \"\"\"Plot training progress\"\"\"\n",
    "        if not self.history['train_loss']:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        epochs = range(1, len(self.history['train_loss']) + 1)\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[0].plot(epochs, self.history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "        axes[0].plot(epochs, self.history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "        axes[0].set_title('Training and Validation Loss', fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        axes[1].plot(epochs, self.history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "        axes[1].plot(epochs, self.history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "        axes[1].set_title('Training and Validation Accuracy', fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Accuracy')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate plot\n",
    "        if self.history['learning_rates']:\n",
    "            axes[2].semilogy(epochs, self.history['learning_rates'], 'g-', linewidth=2)\n",
    "            axes[2].set_title('Learning Rate Schedule', fontweight='bold')\n",
    "            axes[2].set_xlabel('Epoch')\n",
    "            axes[2].set_ylabel('Learning Rate (log scale)')\n",
    "            axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('outputs/visualizations/training_progress.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def train(self, num_epochs=None):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        if num_epochs is None:\n",
    "            num_epochs = self.config['num_epochs']\n",
    "        \n",
    "        print(f\"üöÄ Starting training for {num_epochs} epochs...\")\n",
    "        print(f\"üìä Dataset sizes: Train={len(self.train_loader.dataset)}, Val={len(self.val_loader.dataset)}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc, val_outputs, val_targets = self.validate_epoch()\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step(val_loss)\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Store history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            # Check for improvement\n",
    "            improved = False\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_val_acc = val_acc\n",
    "                self.epochs_without_improvement = 0\n",
    "                improved = True\n",
    "            else:\n",
    "                self.epochs_without_improvement += 1\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f}\")\n",
    "            print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.3f}\")\n",
    "            print(f\"Learning Rate: {current_lr:.2e}\")\n",
    "            print(f\"Best Val Loss: {self.best_val_loss:.4f} | Best Val Acc: {self.best_val_acc:.3f}\")\n",
    "            \n",
    "            if improved:\n",
    "                print(\"üéâ New best model!\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % self.config['save_frequency'] == 0 or improved:\n",
    "                self.save_checkpoint(epoch, is_best=improved)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if self.epochs_without_improvement >= 8:\n",
    "                print(f\"\\\\nüõë Early stopping after {epoch+1} epochs (no improvement for 8 epochs)\")\n",
    "                break\n",
    "            \n",
    "            # Plot progress every few epochs\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.plot_training_progress()\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\\\n‚úÖ Training completed in {training_time/60:.1f} minutes!\")\n",
    "        print(f\"üìà Final Results:\")\n",
    "        print(f\"   Best Validation Loss: {self.best_val_loss:.4f}\")\n",
    "        print(f\"   Best Validation Accuracy: {self.best_val_acc:.3f}\")\n",
    "        \n",
    "        # Final plot\n",
    "        self.plot_training_progress()\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "# Training configuration optimized for Mac\n",
    "training_config = {\n",
    "    'learning_rate': 1e-4,      # Conservative for pretrained model\n",
    "    'weight_decay': 1e-4,       # Regularization\n",
    "    'patience': 3,              # LR scheduler patience  \n",
    "    'min_lr': 1e-7,            # Minimum learning rate\n",
    "    'num_epochs': 20,           # Reasonable for small dataset\n",
    "    'save_frequency': 3         # Save every 3 epochs\n",
    "}\n",
    "\n",
    "# Create data loaders (if datasets exist)\n",
    "if 'train_dataset' in locals() and 'val_dataset' in locals():\n",
    "    print(\"üîÑ Creating data loaders...\")\n",
    "    \n",
    "    # Optimized batch sizes for Mac\n",
    "    batch_size = 16 if device.type == 'mps' else 32\n",
    "    num_workers = 2 if device.type == 'mps' else 4\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if device.type != 'cpu' else False,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if device.type != 'cpu' else False,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data loaders created:\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Num workers: {num_workers}\")\n",
    "    print(f\"   Train batches: {len(train_loader)}\")\n",
    "    print(f\"   Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = SkinConcernTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        config=training_config\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\nüéØ Ready to start training! Run the next cell to begin.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset not available. Please run previous sections to prepare data.\")\n",
    "\n",
    "# Training execution cell (separate for user control)\n",
    "def start_training():\n",
    "    \"\"\"Start the training process\"\"\"\n",
    "    if 'trainer' in locals() or 'trainer' in globals():\n",
    "        print(\"üöÄ Starting training process...\")\n",
    "        history = trainer.train()\n",
    "        return history\n",
    "    else:\n",
    "        print(\"‚ùå Trainer not initialized. Please run the setup cells first.\")\n",
    "        return None\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üéØ TRAINING SETUP COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"To start training, run: history = start_training()\")\n",
    "print(\"Expected training time on Mac M1/M2: ~15-30 minutes\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce80cc84",
   "metadata": {},
   "source": [
    "## üìä Section 7: Model Evaluation and Metrics Calculation\n",
    "\n",
    "Let's evaluate our trained model and calculate comprehensive performance metrics for each skin concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d99c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation metrics for multi-label skin concern detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, concern_labels=None):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.concern_labels = concern_labels or ['acne', 'dark_circles', 'redness', 'wrinkles']\n",
    "    \n",
    "    def evaluate_model(self, data_loader, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation on a dataset\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        all_outputs = []\n",
    "        all_targets = []\n",
    "        all_metadata = []\n",
    "        \n",
    "        print(f\"üîç Evaluating model on {len(data_loader.dataset)} samples...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets, metadata in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "                images = images.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                \n",
    "                all_outputs.extend(outputs.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "                all_metadata.extend(metadata)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        outputs_np = np.array(all_outputs)\n",
    "        targets_np = np.array(all_targets)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self.calculate_comprehensive_metrics(outputs_np, targets_np, threshold)\n",
    "        \n",
    "        return metrics, outputs_np, targets_np, all_metadata\n",
    "    \n",
    "    def calculate_comprehensive_metrics(self, outputs, targets, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive metrics for multi-label classification\n",
    "        \"\"\"\n",
    "        # Binary predictions\n",
    "        predictions = (outputs > threshold).astype(int)\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Overall metrics\n",
    "        overall_accuracy = (predictions == targets).mean()\n",
    "        metrics['overall_accuracy'] = overall_accuracy\n",
    "        \n",
    "        # Per-class metrics\n",
    "        per_class_metrics = {}\n",
    "        \n",
    "        for i, concern in enumerate(self.concern_labels):\n",
    "            y_true = targets[:, i]\n",
    "            y_pred = predictions[:, i]\n",
    "            y_scores = outputs[:, i]\n",
    "            \n",
    "            # Basic metrics\n",
    "            tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "            tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "            fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "            fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            # AUC-ROC\n",
    "            try:\n",
    "                from sklearn.metrics import roc_auc_score\n",
    "                auc = roc_auc_score(y_true, y_scores)\n",
    "            except:\n",
    "                auc = 0.5\n",
    "            \n",
    "            per_class_metrics[concern] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'specificity': specificity,\n",
    "                'f1_score': f1,\n",
    "                'auc_roc': auc,\n",
    "                'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\n",
    "                'support': np.sum(y_true)\n",
    "            }\n",
    "        \n",
    "        metrics['per_class'] = per_class_metrics\n",
    "        \n",
    "        # Macro averages\n",
    "        metrics['macro_precision'] = np.mean([m['precision'] for m in per_class_metrics.values()])\n",
    "        metrics['macro_recall'] = np.mean([m['recall'] for m in per_class_metrics.values()])\n",
    "        metrics['macro_f1'] = np.mean([m['f1_score'] for m in per_class_metrics.values()])\n",
    "        metrics['macro_auc'] = np.mean([m['auc_roc'] for m in per_class_metrics.values()])\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_confusion_matrices(self, outputs, targets, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Plot confusion matrices for each concern\n",
    "        \"\"\"\n",
    "        predictions = (outputs > threshold).astype(int)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, concern in enumerate(self.concern_labels):\n",
    "            y_true = targets[:, i]\n",
    "            y_pred = predictions[:, i]\n",
    "            \n",
    "            # Create confusion matrix\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            \n",
    "            # Plot\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                       xticklabels=['No ' + concern, concern.title()],\n",
    "                       yticklabels=['No ' + concern, concern.title()],\n",
    "                       ax=axes[i])\n",
    "            \n",
    "            axes[i].set_title(f'{concern.replace(\"_\", \" \").title()} Confusion Matrix')\n",
    "            axes[i].set_xlabel('Predicted')\n",
    "            axes[i].set_ylabel('Actual')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('outputs/visualizations/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_metrics_summary(self, metrics):\n",
    "        \"\"\"\n",
    "        Plot comprehensive metrics summary\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        concerns = list(metrics['per_class'].keys())\n",
    "        \n",
    "        # Accuracy by concern\n",
    "        accuracies = [metrics['per_class'][c]['accuracy'] for c in concerns]\n",
    "        axes[0,0].bar(concerns, accuracies, color='skyblue', alpha=0.8)\n",
    "        axes[0,0].set_title('Accuracy by Concern', fontweight='bold')\n",
    "        axes[0,0].set_ylabel('Accuracy')\n",
    "        axes[0,0].set_ylim(0, 1)\n",
    "        for i, v in enumerate(accuracies):\n",
    "            axes[0,0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "        \n",
    "        # Precision, Recall, F1\n",
    "        precisions = [metrics['per_class'][c]['precision'] for c in concerns]\n",
    "        recalls = [metrics['per_class'][c]['recall'] for c in concerns]\n",
    "        f1_scores = [metrics['per_class'][c]['f1_score'] for c in concerns]\n",
    "        \n",
    "        x = np.arange(len(concerns))\n",
    "        width = 0.25\n",
    "        \n",
    "        axes[0,1].bar(x - width, precisions, width, label='Precision', alpha=0.8)\n",
    "        axes[0,1].bar(x, recalls, width, label='Recall', alpha=0.8)\n",
    "        axes[0,1].bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "        \n",
    "        axes[0,1].set_title('Precision, Recall, F1-Score by Concern', fontweight='bold')\n",
    "        axes[0,1].set_ylabel('Score')\n",
    "        axes[0,1].set_xticks(x)\n",
    "        axes[0,1].set_xticklabels([c.replace('_', ' ').title() for c in concerns])\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].set_ylim(0, 1)\n",
    "        \n",
    "        # AUC-ROC\n",
    "        aucs = [metrics['per_class'][c]['auc_roc'] for c in concerns]\n",
    "        axes[1,0].bar(concerns, aucs, color='lightcoral', alpha=0.8)\n",
    "        axes[1,0].set_title('AUC-ROC by Concern', fontweight='bold')\n",
    "        axes[1,0].set_ylabel('AUC-ROC')\n",
    "        axes[1,0].set_ylim(0, 1)\n",
    "        axes[1,0].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "        for i, v in enumerate(aucs):\n",
    "            axes[1,0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "        \n",
    "        # Support (number of positive samples)\n",
    "        supports = [metrics['per_class'][c]['support'] for c in concerns]\n",
    "        axes[1,1].bar(concerns, supports, color='lightgreen', alpha=0.8)\n",
    "        axes[1,1].set_title('Positive Samples by Concern', fontweight='bold')\n",
    "        axes[1,1].set_ylabel('Count')\n",
    "        for i, v in enumerate(supports):\n",
    "            axes[1,1].text(i, v + 1, str(v), ha='center', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('outputs/visualizations/metrics_summary.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def print_evaluation_report(self, metrics):\n",
    "        \"\"\"\n",
    "        Print comprehensive evaluation report\n",
    "        \"\"\"\n",
    "        print(\"\\\\n\" + \"=\"*80)\n",
    "        print(\"üéØ COMPREHENSIVE EVALUATION REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\\\nüìä Overall Performance:\")\n",
    "        print(f\"   Overall Accuracy: {metrics['overall_accuracy']:.3f}\")\n",
    "        print(f\"   Macro Precision:  {metrics['macro_precision']:.3f}\")\n",
    "        print(f\"   Macro Recall:     {metrics['macro_recall']:.3f}\")\n",
    "        print(f\"   Macro F1-Score:   {metrics['macro_f1']:.3f}\")\n",
    "        print(f\"   Macro AUC-ROC:    {metrics['macro_auc']:.3f}\")\n",
    "        \n",
    "        print(f\"\\\\nüìã Per-Class Performance:\")\n",
    "        print(f\"{'Concern':<15} {'Acc':<6} {'Prec':<6} {'Rec':<6} {'F1':<6} {'AUC':<6} {'Supp':<6}\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        for concern, metrics_dict in metrics['per_class'].items():\n",
    "            print(f\"{concern.replace('_', ' ').title():<15} \"\n",
    "                  f\"{metrics_dict['accuracy']:<6.3f} \"\n",
    "                  f\"{metrics_dict['precision']:<6.3f} \"\n",
    "                  f\"{metrics_dict['recall']:<6.3f} \"\n",
    "                  f\"{metrics_dict['f1_score']:<6.3f} \"\n",
    "                  f\"{metrics_dict['auc_roc']:<6.3f} \"\n",
    "                  f\"{metrics_dict['support']:<6}\")\n",
    "        \n",
    "        print(\"\\\\n\" + \"=\"*80)\n",
    "\n",
    "# Load best model if available\n",
    "def load_best_model():\n",
    "    \"\"\"Load the best trained model\"\"\"\n",
    "    model_path = 'models/saved_weights/best_model.pth'\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"üì• Loading best model from {model_path}...\")\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # Load model state\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded successfully!\")\n",
    "        print(f\"   Trained for {checkpoint['epoch']+1} epochs\")\n",
    "        print(f\"   Best validation loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "        print(f\"   Best validation accuracy: {checkpoint['best_val_acc']:.3f}\")\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No trained model found at {model_path}\")\n",
    "        print(f\"   Please train the model first or check the path.\")\n",
    "        return False\n",
    "\n",
    "# Evaluation execution\n",
    "def run_comprehensive_evaluation():\n",
    "    \"\"\"Run comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    # Load best model\n",
    "    if not load_best_model():\n",
    "        return None\n",
    "    \n",
    "    # Create evaluator\n",
    "    evaluator = ModelEvaluator(model, device, model.concern_labels)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    if 'val_loader' in locals() or 'val_loader' in globals():\n",
    "        print(f\"\\\\nüîç Evaluating on validation set...\")\n",
    "        val_metrics, val_outputs, val_targets, val_metadata = evaluator.evaluate_model(val_loader)\n",
    "        \n",
    "        # Print report\n",
    "        evaluator.print_evaluation_report(val_metrics)\n",
    "        \n",
    "        # Plot visualizations\n",
    "        print(f\"\\\\nüìä Creating evaluation visualizations...\")\n",
    "        evaluator.plot_confusion_matrices(val_outputs, val_targets)\n",
    "        evaluator.plot_metrics_summary(val_metrics)\n",
    "        \n",
    "        return val_metrics, evaluator\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Validation data loader not available\")\n",
    "        return None, evaluator\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUATION SETUP COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"To run comprehensive evaluation, execute:\")\n",
    "print(\"val_metrics, evaluator = run_comprehensive_evaluation()\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83705de",
   "metadata": {},
   "source": [
    "## üî• Section 8: GradCAM Implementation for Explainable AI\n",
    "\n",
    "Now let's implement GradCAM (Gradient-weighted Class Activation Mapping) to visualize which facial regions influence each prediction, providing explainable AI capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelGradCAM:\n",
    "    \"\"\"\n",
    "    GradCAM implementation for multi-label skin concern classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layer_name='layer4'):\n",
    "        \"\"\"\n",
    "        Initialize GradCAM\n",
    "        \n",
    "        Args:\n",
    "            model: Trained SkinConcernDetector model\n",
    "            target_layer_name: Name of the target layer for gradient extraction\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Get the target layer (ResNet18's layer4)\n",
    "        self.target_layer = getattr(self.model.backbone, target_layer_name)\n",
    "        \n",
    "        # Hooks for storing gradients and activations\n",
    "        self.gradients = {}\n",
    "        self.activations = {}\n",
    "        \n",
    "        # Register hooks\n",
    "        self.target_layer.register_forward_hook(self._save_activation)\n",
    "        self.target_layer.register_full_backward_hook(self._save_gradient)\n",
    "        \n",
    "        self.concern_labels = model.concern_labels\n",
    "        \n",
    "        print(f\"üî• GradCAM initialized for {target_layer_name}\")\n",
    "        print(f\"   Target layer output shape: Will be determined during forward pass\")\n",
    "        print(f\"   Concerns: {', '.join(self.concern_labels)}\")\n",
    "    \n",
    "    def _save_activation(self, module, input, output):\n",
    "        \"\"\"Hook to save forward activations\"\"\"\n",
    "        self.activations['value'] = output.detach()\n",
    "    \n",
    "    def _save_gradient(self, module, grad_input, grad_output):\n",
    "        \"\"\"Hook to save backward gradients\"\"\"\n",
    "        self.gradients['value'] = grad_output[0].detach()\n",
    "    \n",
    "    def generate_cam(self, input_tensor, target_class_idx):\n",
    "        \"\"\"\n",
    "        Generate Class Activation Map for a specific concern\n",
    "        \n",
    "        Args:\n",
    "            input_tensor: Input image tensor [1, 3, 224, 224]\n",
    "            target_class_idx: Index of target concern (0-3)\n",
    "            \n",
    "        Returns:\n",
    "            CAM heatmap as numpy array [H, W]\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_tensor)\n",
    "        \n",
    "        # Get the score for target class\n",
    "        class_score = output[0, target_class_idx]\n",
    "        \n",
    "        # Backward pass\n",
    "        class_score.backward(retain_graph=True)\n",
    "        \n",
    "        # Get gradients and activations\n",
    "        gradients = self.gradients['value'][0]  # [C, H, W]\n",
    "        activations = self.activations['value'][0]  # [C, H, W]\n",
    "        \n",
    "        # Global average pooling of gradients to get weights\n",
    "        weights = torch.mean(gradients, dim=(1, 2))  # [C]\n",
    "        \n",
    "        # Weighted combination of activation maps\n",
    "        cam = torch.zeros(activations.shape[1:], dtype=torch.float32)  # [H, W]\n",
    "        \n",
    "        for i, weight in enumerate(weights):\n",
    "            cam += weight * activations[i]\n",
    "        \n",
    "        # Apply ReLU (only positive influences)\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        # Normalize CAM to [0, 1]\n",
    "        if cam.max() > 0:\n",
    "            cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "        \n",
    "        return cam.cpu().numpy()\n",
    "    \n",
    "    def generate_all_cams(self, input_tensor, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Generate CAMs for all concerns with predictions and scores\n",
    "        \n",
    "        Args:\n",
    "            input_tensor: Input image tensor [1, 3, 224, 224]\n",
    "            threshold: Classification threshold\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with CAMs, scores, and predictions for each concern\n",
    "        \"\"\"\n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(input_tensor)\n",
    "            scores = predictions[0].cpu().numpy()\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for i, concern in enumerate(self.concern_labels):\n",
    "            # Generate CAM for this concern\n",
    "            cam = self.generate_cam(input_tensor, i)\n",
    "            \n",
    "            # Store results\n",
    "            results[concern] = {\n",
    "                'cam': cam,\n",
    "                'score': float(scores[i]),\n",
    "                'predicted': scores[i] > threshold,\n",
    "                'confidence_level': 'High' if abs(scores[i] - 0.5) > 0.3 else 'Medium' if abs(scores[i] - 0.5) > 0.1 else 'Low'\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def apply_colormap_and_overlay(self, original_image, cam, alpha=0.4, colormap=cv2.COLORMAP_JET):\n",
    "        \"\"\"\n",
    "        Apply colormap to CAM and overlay on original image\n",
    "        \n",
    "        Args:\n",
    "            original_image: Original PIL image\n",
    "            cam: CAM heatmap [H, W]\n",
    "            alpha: Overlay transparency\n",
    "            colormap: OpenCV colormap\n",
    "            \n",
    "        Returns:\n",
    "            Overlayed image as numpy array\n",
    "        \"\"\"\n",
    "        # Resize CAM to match image size\n",
    "        h, w = original_image.size[1], original_image.size[0]  # PIL size is (width, height)\n",
    "        cam_resized = cv2.resize(cam, (w, h))\n",
    "        \n",
    "        # Apply colormap\n",
    "        cam_colored = cv2.applyColorMap(np.uint8(255 * cam_resized), colormap)\n",
    "        cam_colored = cv2.cvtColor(cam_colored, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert original image to numpy\n",
    "        original_np = np.array(original_image)\n",
    "        \n",
    "        # Overlay\n",
    "        overlayed = cam_colored * alpha + original_np * (1 - alpha)\n",
    "        \n",
    "        return np.uint8(overlayed)\n",
    "    \n",
    "    def visualize_all_concerns(self, original_image, input_tensor, threshold=0.5, save_path=None):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualization for all concerns\n",
    "        \n",
    "        Args:\n",
    "            original_image: Original PIL image\n",
    "            input_tensor: Preprocessed input tensor\n",
    "            threshold: Classification threshold\n",
    "            save_path: Optional path to save visualization\n",
    "            \n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        # Generate CAMs\n",
    "        results = self.generate_all_cams(input_tensor, threshold)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Original image\n",
    "        axes[0, 0].imshow(original_image)\n",
    "        axes[0, 0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Summary of predictions\n",
    "        detected_concerns = [concern for concern, data in results.items() if data['predicted']]\n",
    "        summary_text = f\"Detected Concerns: {len(detected_concerns)}\\\\n\"\n",
    "        \n",
    "        for concern, data in results.items():\n",
    "            status = \"‚úÖ DETECTED\" if data['predicted'] else \"‚ùå Not Detected\"\n",
    "            confidence = data['confidence_level']\n",
    "            summary_text += f\"{concern.replace('_', ' ').title()}: {data['score']:.1%} ({confidence}) {status}\\\\n\"\n",
    "        \n",
    "        axes[0, 1].text(0.1, 0.5, summary_text, fontsize=11, verticalalignment='center',\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\"))\n",
    "        axes[0, 1].set_xlim(0, 1)\n",
    "        axes[0, 1].set_ylim(0, 1)\n",
    "        axes[0, 1].set_title('Prediction Summary', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # Individual concern visualizations\n",
    "        positions = [(0, 2), (1, 0), (1, 1), (1, 2)]\n",
    "        colors = ['red', 'blue', 'orange', 'purple']\n",
    "        \n",
    "        for idx, (concern, data) in enumerate(results.items()):\n",
    "            row, col = positions[idx]\n",
    "            \n",
    "            # Create overlay\n",
    "            overlay = self.apply_colormap_and_overlay(\n",
    "                original_image, \n",
    "                data['cam'], \n",
    "                alpha=0.4,\n",
    "                colormap=cv2.COLORMAP_JET\n",
    "            )\n",
    "            \n",
    "            axes[row, col].imshow(overlay)\n",
    "            \n",
    "            # Title with prediction info\n",
    "            title_color = 'green' if data['predicted'] else 'red'\n",
    "            confidence_emoji = \"üî•\" if data['confidence_level'] == 'High' else \"üü°\" if data['confidence_level'] == 'Medium' else \"üîµ\"\n",
    "            \n",
    "            title = f\"{confidence_emoji} {concern.replace('_', ' ').title()}\\\\n{data['score']:.1%} ({data['confidence_level']})\"\n",
    "            \n",
    "            axes[row, col].set_title(title, fontsize=12, fontweight='bold', color=title_color)\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.suptitle('Face Concern Detection with GradCAM Visualization', \n",
    "                    fontsize=16, fontweight='bold', y=0.95)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"üíæ Visualization saved to {save_path}\")\n",
    "        \n",
    "        return fig, results\n",
    "\n",
    "class FaceConcernPredictor:\n",
    "    \"\"\"\n",
    "    Complete end-to-end predictor with face detection, classification, and visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, device, face_preprocessor=None):\n",
    "        \"\"\"\n",
    "        Initialize predictor with trained model\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        \n",
    "        # Load model\n",
    "        self.model = SkinConcernDetector(num_classes=4).to(device)\n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path, map_location=device)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"‚úÖ Model loaded from {model_path}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Model file not found: {model_path}\")\n",
    "            print(\"   Using randomly initialized weights\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Face preprocessor\n",
    "        self.face_preprocessor = face_preprocessor or FacePreprocessor()\n",
    "        \n",
    "        # Data transform (same as validation)\n",
    "        self.transform = create_data_transforms(train=False)\n",
    "        \n",
    "        # GradCAM\n",
    "        self.gradcam = MultiLabelGradCAM(self.model)\n",
    "        \n",
    "        print(f\"üéØ FaceConcernPredictor initialized and ready!\")\n",
    "    \n",
    "    def predict_single_image(self, image_path, threshold=0.5, visualize=True, save_path=None):\n",
    "        \"\"\"\n",
    "        Complete prediction pipeline for a single image\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to image file\n",
    "            threshold: Classification threshold\n",
    "            visualize: Whether to create GradCAM visualization\n",
    "            save_path: Optional path to save results\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with predictions and visualizations\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            original_image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            # Face detection and preprocessing\n",
    "            face_image, detection_info = self.face_preprocessor.preprocess_image(original_image)\n",
    "            \n",
    "            if face_image is None:\n",
    "                return {\n",
    "                    'error': f'Face detection failed: {detection_info}',\n",
    "                    'image_path': image_path\n",
    "                }\n",
    "            \n",
    "            # Prepare input tensor\n",
    "            input_tensor = self.transform(face_image).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions = self.model(input_tensor)\n",
    "                scores = predictions[0].cpu().numpy()\n",
    "            \n",
    "            # Prepare results\n",
    "            results = {\n",
    "                'image_path': image_path,\n",
    "                'face_detection': detection_info,\n",
    "                'predictions': {},\n",
    "                'detected_concerns': []\n",
    "            }\n",
    "            \n",
    "            for i, concern in enumerate(self.model.concern_labels):\n",
    "                score = float(scores[i])\n",
    "                predicted = score > threshold\n",
    "                \n",
    "                results['predictions'][concern] = {\n",
    "                    'score': score,\n",
    "                    'predicted': predicted,\n",
    "                    'confidence': score\n",
    "                }\n",
    "                \n",
    "                if predicted:\n",
    "                    results['detected_concerns'].append(concern)\n",
    "            \n",
    "            # Generate visualization if requested\n",
    "            if visualize:\n",
    "                fig, gradcam_results = self.gradcam.visualize_all_concerns(\n",
    "                    face_image, input_tensor, threshold, save_path\n",
    "                )\n",
    "                \n",
    "                results['visualization'] = fig\n",
    "                results['gradcam_results'] = gradcam_results\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': f'Prediction failed: {str(e)}',\n",
    "                'image_path': image_path\n",
    "            }\n",
    "\n",
    "# Test GradCAM functionality\n",
    "def test_gradcam():\n",
    "    \"\"\"Test GradCAM on a sample image\"\"\"\n",
    "    \n",
    "    # Create sample input\n",
    "    sample_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    \n",
    "    # Initialize GradCAM\n",
    "    gradcam = MultiLabelGradCAM(model)\n",
    "    \n",
    "    print(\"üß™ Testing GradCAM functionality...\")\n",
    "    \n",
    "    # Test CAM generation\n",
    "    try:\n",
    "        cam = gradcam.generate_cam(sample_input, target_class_idx=0)\n",
    "        print(f\"‚úÖ CAM generation successful!\")\n",
    "        print(f\"   CAM shape: {cam.shape}\")\n",
    "        print(f\"   CAM range: [{cam.min():.3f}, {cam.max():.3f}]\")\n",
    "        \n",
    "        # Test all CAMs\n",
    "        results = gradcam.generate_all_cams(sample_input)\n",
    "        print(f\"‚úÖ Multi-label CAM generation successful!\")\n",
    "        print(f\"   Generated CAMs for: {list(results.keys())}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GradCAM test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test the implementation\n",
    "print(\"üî• Testing GradCAM implementation...\")\n",
    "if test_gradcam():\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"üéØ GRADCAM SETUP COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"GradCAM is ready for generating explainable visualizations!\")\n",
    "    print(\"Use the FaceConcernPredictor class for end-to-end inference.\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚ùå GradCAM setup failed. Please check the model and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47962428",
   "metadata": {},
   "source": [
    "## üöÄ Section 9: Inference Pipeline with Visual Overlays\n",
    "\n",
    "Let's create the complete inference pipeline that processes new images and generates confidence scores with semi-transparent overlay masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6212fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the predictor\n",
    "model_path = 'models/saved_weights/best_model.pth'\n",
    "\n",
    "# Create predictor instance\n",
    "if os.path.exists(model_path):\n",
    "    predictor = FaceConcernPredictor(\n",
    "        model_path=model_path,\n",
    "        device=device,\n",
    "        face_preprocessor=face_preprocessor\n",
    "    )\n",
    "    print(\"‚úÖ Predictor initialized with trained model\")\n",
    "else:\n",
    "    # Use untrained model for demonstration\n",
    "    predictor = FaceConcernPredictor(\n",
    "        model_path='',  # Empty path will use random weights\n",
    "        device=device,\n",
    "        face_preprocessor=face_preprocessor\n",
    "    )\n",
    "    print(\"‚ö†Ô∏è Using untrained model for demonstration\")\n",
    "\n",
    "def create_sample_test_images():\n",
    "    \"\"\"\n",
    "    Create sample test images from our dataset for demonstration\n",
    "    \"\"\"\n",
    "    sample_dir = 'data/sample_images'\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy a few sample images from processed dataset\n",
    "    processed_dir = 'data/processed'\n",
    "    \n",
    "    if os.path.exists(processed_dir):\n",
    "        # Get some sample images\n",
    "        all_images = [f for f in os.listdir(processed_dir) \n",
    "                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        # Sample images from different categories\n",
    "        sample_images = []\n",
    "        \n",
    "        # Try to get diverse samples\n",
    "        for prefix in ['ds1_acne', 'ds1_wrinkles', 'ds1_spots', 'ds2_acne', 'ds2_redness']:\n",
    "            matching = [img for img in all_images if img.startswith(prefix)]\n",
    "            if matching:\n",
    "                sample_images.append(matching[0])\n",
    "        \n",
    "        # Add a few more random samples\n",
    "        remaining = [img for img in all_images if img not in sample_images]\n",
    "        sample_images.extend(remaining[:3])\n",
    "        \n",
    "        # Copy to sample directory\n",
    "        copied_samples = []\n",
    "        for i, img in enumerate(sample_images[:6]):  # Limit to 6 samples\n",
    "            src_path = os.path.join(processed_dir, img)\n",
    "            dst_path = os.path.join(sample_dir, f'sample_{i+1}_{img}')\n",
    "            \n",
    "            if os.path.exists(src_path):\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "                copied_samples.append(dst_path)\n",
    "        \n",
    "        print(f\"üì∏ Created {len(copied_samples)} sample test images in {sample_dir}\")\n",
    "        return copied_samples\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No processed images available for sampling\")\n",
    "        return []\n",
    "\n",
    "def batch_inference_demo(image_paths, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Demonstrate batch inference on multiple images\n",
    "    \"\"\"\n",
    "    print(f\"üîç Running batch inference on {len(image_paths)} images...\")\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    # Process each image\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        print(f\"\\\\nProcessing image {i+1}/{len(image_paths)}: {os.path.basename(img_path)}\")\n",
    "        \n",
    "        # Run prediction\n",
    "        result = predictor.predict_single_image(\n",
    "            img_path, \n",
    "            threshold=threshold, \n",
    "            visualize=True,\n",
    "            save_path=f'outputs/visualizations/inference_result_{i+1}.png'\n",
    "        )\n",
    "        \n",
    "        if 'error' in result:\n",
    "            print(f\"‚ùå {result['error']}\")\n",
    "            continue\n",
    "        \n",
    "        # Summary\n",
    "        detected = result['detected_concerns']\n",
    "        total_score = sum(data['score'] for data in result['predictions'].values())\n",
    "        \n",
    "        summary = {\n",
    "            'image': os.path.basename(img_path),\n",
    "            'detected_concerns': len(detected),\n",
    "            'concerns_list': detected,\n",
    "            'average_confidence': total_score / 4,\n",
    "            'face_confidence': result['face_detection']['confidence']\n",
    "        }\n",
    "        \n",
    "        results_summary.append(summary)\n",
    "        \n",
    "        print(f\"‚úÖ Detected {len(detected)} concerns: {', '.join(detected)}\")\n",
    "        print(f\"   Average confidence: {summary['average_confidence']:.1%}\")\n",
    "        print(f\"   Face detection confidence: {summary['face_confidence']:.2f}\")\n",
    "    \n",
    "    return results_summary\n",
    "\n",
    "def create_batch_summary_visualization(results_summary):\n",
    "    \"\"\"\n",
    "    Create summary visualization of batch results\n",
    "    \"\"\"\n",
    "    if not results_summary:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Number of detected concerns per image\n",
    "    images = [r['image'] for r in results_summary]\n",
    "    concern_counts = [r['detected_concerns'] for r in results_summary]\n",
    "    \n",
    "    axes[0,0].bar(range(len(images)), concern_counts, color='skyblue', alpha=0.8)\n",
    "    axes[0,0].set_title('Detected Concerns per Image', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Number of Concerns')\n",
    "    axes[0,0].set_xticks(range(len(images)))\n",
    "    axes[0,0].set_xticklabels([f'Img {i+1}' for i in range(len(images))], rotation=45)\n",
    "    \n",
    "    for i, v in enumerate(concern_counts):\n",
    "        axes[0,0].text(i, v + 0.1, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    # Average confidence scores\n",
    "    avg_confidences = [r['average_confidence'] for r in results_summary]\n",
    "    axes[0,1].bar(range(len(images)), avg_confidences, color='lightcoral', alpha=0.8)\n",
    "    axes[0,1].set_title('Average Confidence Scores', fontweight='bold')\n",
    "    axes[0,1].set_ylabel('Confidence')\n",
    "    axes[0,1].set_ylim(0, 1)\n",
    "    axes[0,1].set_xticks(range(len(images)))\n",
    "    axes[0,1].set_xticklabels([f'Img {i+1}' for i in range(len(images))], rotation=45)\n",
    "    \n",
    "    # Face detection confidence\n",
    "    face_confidences = [r['face_confidence'] for r in results_summary]\n",
    "    axes[1,0].bar(range(len(images)), face_confidences, color='lightgreen', alpha=0.8)\n",
    "    axes[1,0].set_title('Face Detection Confidence', fontweight='bold')\n",
    "    axes[1,0].set_ylabel('Confidence')\n",
    "    axes[1,0].set_ylim(0, 1)\n",
    "    axes[1,0].set_xticks(range(len(images)))\n",
    "    axes[1,0].set_xticklabels([f'Img {i+1}' for i in range(len(images))], rotation=45)\n",
    "    \n",
    "    # Concern distribution across all images\n",
    "    all_concerns = []\n",
    "    for r in results_summary:\n",
    "        all_concerns.extend(r['concerns_list'])\n",
    "    \n",
    "    from collections import Counter\n",
    "    concern_counts = Counter(all_concerns)\n",
    "    \n",
    "    if concern_counts:\n",
    "        concerns = list(concern_counts.keys())\n",
    "        counts = list(concern_counts.values())\n",
    "        \n",
    "        axes[1,1].bar(concerns, counts, color='gold', alpha=0.8)\n",
    "        axes[1,1].set_title('Overall Concern Distribution', fontweight='bold')\n",
    "        axes[1,1].set_ylabel('Frequency')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        for i, v in enumerate(counts):\n",
    "            axes[1,1].text(i, v + 0.1, str(v), ha='center', fontweight='bold')\n",
    "    else:\n",
    "        axes[1,1].text(0.5, 0.5, 'No concerns detected\\\\nacross all images', \n",
    "                      ha='center', va='center', fontsize=12)\n",
    "        axes[1,1].set_xlim(0, 1)\n",
    "        axes[1,1].set_ylim(0, 1)\n",
    "        axes[1,1].set_title('Overall Concern Distribution', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/visualizations/batch_inference_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def interactive_single_prediction():\n",
    "    \"\"\"\n",
    "    Interactive function for single image prediction\n",
    "    \"\"\"\n",
    "    print(\"üéØ Interactive Single Image Prediction\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get available sample images\n",
    "    sample_dir = 'data/sample_images'\n",
    "    if os.path.exists(sample_dir):\n",
    "        sample_images = [f for f in os.listdir(sample_dir) \n",
    "                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        if sample_images:\n",
    "            print(f\"Available sample images:\")\n",
    "            for i, img in enumerate(sample_images):\n",
    "                print(f\"  {i+1}. {img}\")\n",
    "            \n",
    "            try:\n",
    "                choice = int(input(f\"\\\\nSelect image (1-{len(sample_images)}): \")) - 1\n",
    "                if 0 <= choice < len(sample_images):\n",
    "                    selected_image = os.path.join(sample_dir, sample_images[choice])\n",
    "                    \n",
    "                    threshold = float(input(\"Enter threshold (0.0-1.0, default 0.5): \") or \"0.5\")\n",
    "                    \n",
    "                    print(f\"\\\\nüîç Processing {sample_images[choice]}...\")\n",
    "                    \n",
    "                    result = predictor.predict_single_image(\n",
    "                        selected_image,\n",
    "                        threshold=threshold,\n",
    "                        visualize=True,\n",
    "                        save_path='outputs/visualizations/interactive_result.png'\n",
    "                    )\n",
    "                    \n",
    "                    if 'error' in result:\n",
    "                        print(f\"‚ùå {result['error']}\")\n",
    "                        return\n",
    "                    \n",
    "                    # Display results\n",
    "                    print(f\"\\\\nüéâ Results for {sample_images[choice]}:\")\n",
    "                    print(f\"Face Detection Confidence: {result['face_detection']['confidence']:.2f}\")\n",
    "                    print(f\"\\\\nSkin Concern Predictions:\")\n",
    "                    \n",
    "                    for concern, data in result['predictions'].items():\n",
    "                        status = \"‚úÖ DETECTED\" if data['predicted'] else \"‚ùå Not Detected\"\n",
    "                        print(f\"  {concern.replace('_', ' ').title()}: {data['score']:.1%} {status}\")\n",
    "                    \n",
    "                    if result['detected_concerns']:\n",
    "                        print(f\"\\\\nüî• Summary: Detected {len(result['detected_concerns'])} concerns\")\n",
    "                        print(f\"Concerns: {', '.join(result['detected_concerns'])}\")\n",
    "                    else:\n",
    "                        print(f\"\\\\n‚ú® Summary: No skin concerns detected above threshold\")\n",
    "                    \n",
    "                    plt.show()  # Show the visualization\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Invalid selection\")\n",
    "                    \n",
    "            except ValueError:\n",
    "                print(\"Invalid input\")\n",
    "        else:\n",
    "            print(\"No sample images available\")\n",
    "    else:\n",
    "        print(\"Sample images directory not found\")\n",
    "\n",
    "# Create sample images for testing\n",
    "sample_paths = create_sample_test_images()\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üöÄ INFERENCE PIPELINE READY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Available functions:\")\n",
    "print(\"1. batch_inference_demo(sample_paths) - Run batch inference\")\n",
    "print(\"2. interactive_single_prediction() - Interactive single prediction\")\n",
    "print(\"3. predictor.predict_single_image(path) - Direct prediction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Demo execution\n",
    "if sample_paths:\n",
    "    print(f\"\\\\nüéØ Running batch inference demo on {len(sample_paths)} sample images...\")\n",
    "    \n",
    "    # Run batch inference\n",
    "    results = batch_inference_demo(sample_paths, threshold=0.5)\n",
    "    \n",
    "    # Create summary visualization\n",
    "    if results:\n",
    "        print(f\"\\\\nüìä Creating batch summary visualization...\")\n",
    "        create_batch_summary_visualization(results)\n",
    "        \n",
    "        print(f\"\\\\n‚úÖ Batch inference complete!\")\n",
    "        print(f\"   Processed: {len(results)} images\")\n",
    "        print(f\"   Average concerns per image: {np.mean([r['detected_concerns'] for r in results]):.1f}\")\n",
    "        print(f\"   Results saved to: outputs/visualizations/\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\\\n‚ö†Ô∏è No sample images available for demonstration\")\n",
    "    print(\"Please ensure datasets are downloaded and processed first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42220d90",
   "metadata": {},
   "source": [
    "## üéÆ Section 10: Interactive Demo & Real-Time Analysis\n",
    "\n",
    "This final section provides an interactive demonstration of the complete face concern detection system with various testing scenarios and real-time capabilities."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
